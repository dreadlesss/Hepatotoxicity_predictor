import os
import gc
import time
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from copy import deepcopy
import shutil

from boruta import BorutaPy
from rdkit import Chem
from rdkit.Chem import Descriptors
from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect
from rdkit.Chem.AllChem import GetErGFingerprint
from rdkit.Chem import RDKFingerprint
from rdkit.Chem.AtomPairs import Pairs
from rdkit.Chem.AtomPairs import Torsions
from rdkit.Chem import MACCSkeys
from rdkit.Chem.EState.Fingerprinter import FingerprintMol
from rdkit.Avalon.pyAvalonTools import GetAvalonFP
from rdkit.ML.Descriptors import MoleculeDescriptors

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import RFECV
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier

__all__ = ['load_object', 'Data', 'MolecularData']


def load_object(fname):
    '''reload the object and results.

    Parameters
    ----------
    fname : str, default=None
        The name of the file.
        
    Returns
    -------
    results_obj : object
        The data object.
    '''
    pkl_file = open(fname, 'rb')
    results_obj = pickle.load(pkl_file)
    pkl_file.close()
    return results_obj


def standardize(scaler, x_fit=None, x_transform=None):
    '''Features standardization.
    
    Parameters
    ----------
    scaler : string {'MinMaxScaler', 'StandardScaler', 'Normalizer'}
        or callable {StandardScaler(), MinMaxScaler(), Normalizer()}
        The method that applied to the features.
        
    x_fit : array-like
        The data with at least one none zero component to train the scaler.
        
    x_transform : array-like
        The data to be transformed by scaler.
    
    Returns
    -------
    scaler : object
        The scaler object.
        
    x_output : array-like
        The transformed data.
    '''
    # initialize the scaler
    if isinstance(scaler, str):
        if scaler == 'MinMaxScaler':
            scaler = MinMaxScaler()
        elif scaler == 'StandardScaler':
            scaler = StandardScaler()
        elif scaler == 'Normalizer':
            scaler = Normalizer()

    # fit or transform
    if isinstance(scaler, (StandardScaler, MinMaxScaler, Normalizer)):
        if x_fit is not None:
            scaler.fit(x_fit)
        if x_transform is None:
            x_output = scaler.transform(x_fit)
        else:
            x_output = scaler.fit_transform(x_transform)
    return scaler, x_output


def smiles2rdmol(df, smiles_col):
    '''Convert the smiles to the rdmol object.
    Return the column name of the rdmol.

    Parameters
    ----------
    df : dataframe object
        Dataframe contains the molecules.
        
    smiles_col : string
        Column contains the smiles.
        
    Returns
    -------
    col_name : str
        The name of the rdmol column.
    '''
    # set the name for the rdmol column
    col_prefix = 'rdmol'
    col_name = col_prefix
    i = 0
    while col_name in df.columns:
        i += 1
        col_name = '%s_%s' % (col_prefix, i)

    # convert smiles to rdmol
    df[col_name] = df[smiles_col].map(Chem.MolFromSmiles)
    df_error = df[df[col_name] != df[col_name]][smiles_col]

    # try to set charge for fail molecules
    for index, smiles in df_error.items():
        m = Chem.MolFromSmiles(smiles, sanitize=False)
        try:
            new_smiles = Chem.MolToSmiles(Chem.RemoveHs(m), isomericSmiles=True)
            df.loc[index, smiles_col] = new_smiles
            m = Chem.MolFromSmiles(new_smiles)
            df.loc[index, col_name] = m
        except:
            df.loc[index, col_name] = None
    return col_name


def fingerprint_generator(mol, fp_type='morgan', **fp_params):
    '''Generate fingerprints from smiles by rdkit
        
    Parameters
    ----------
    mol : rdmol object
        The molecules object generated by rdkit.
        
    fp_type : string
        The type of fingerprint (FP).
        
    fp_params : dict
        Parameters of the function that used to calculate the FP.
        
    Returns
    -------
    fp : fingerprints
        The calculated fingerprints.
    '''
    if fp_type == 'morgan':
        fp = GetMorganFingerprintAsBitVect(mol, **fp_params)

    elif fp_type == 'RDK':
        fp = RDKFingerprint(mol, **fp_params)

    elif fp_type == 'pairs':
        fp = Pairs.GetHashedAtomPairFingerprint(mol, **fp_params)

    elif fp_type == 'torsion':
        fp = Torsions.GetHashedTopologicalTorsionFingerprint(mol, **fp_params)

    elif fp_type == 'avalon':
        fp = GetAvalonFP(mol, **fp_params)

    elif fp_type == 'EState':
        fp = FingerprintMol(mol, **fp_params)[0]

    elif fp_type == 'ErG':
        fp = GetErGFingerprint(mol, **fp_params)
        
    elif fp_type == 'MACCS':
        fp = MACCSkeys.GenMACCSKeys(mol, **fp_params)

    return fp


class Data():
    '''Load data and perform feature selection for the training and predicted data.

    Parameters
    ----------
    n_jobs : int, default=-1
        The number of cpu that will be used.
        
    data_path : str or path object
        The folder used to save the data.
    '''

    def __init__(self, n_jobs=-1, data_path='./data'):
        if n_jobs < 1:
            self.n_jobs = os.cpu_count()
        else:
            self.n_jobs = n_jobs
        self._train_data_exist = False
        self._predict_data_exist = False
        self._split = False
        self.data_path = Path(data_path)
        self.feature_expand = []
        self.feature_expand_params = {}

    def _valid_file(self, fname):
        '''Check if the file exists.
        
        Parameters
        ----------
        fname : str or path object
            The file that will be used.
            
        Returns
        -------
        file_path : path object
            The valid file path.
        '''
        print(fname)
        path = Path(fname)
        file_path = Path('./data') / path.name
        if not file_path.exists():
            alter_path = Path(path.name)
            if alter_path.exists():
                shutil.move(alter_path, file_path)
            else:
                raise Exception('Can not find the %s' % fname)
        return file_path

    def _file_preprocess(self, file_path, drop_col=None):
        '''Read the file, drop the useless columns, return the data.
        
        Parameters
        ----------
        file_path : str
            File path contains the data.
            
        drop_col : str
            The useless columns that will be delete.
            
        Returns
        -------
        df : DataFrame
            The data (df) ready to use.
            
        path : path object
            The valid path of the data file.
        '''
        # load the file
        path = self._valid_file(file_path)
        if path.suffix == '.xlsx':
            df = pd.read_excel(path)
        elif path.suffix == '.tsv':
            df = pd.read_csv(path, delimiter='\t')
        elif path.suffix == '.csv':
            df = pd.read_csv(path, delimiter=',')

        # delete the usless columns
        if drop_col:
            df.drop(drop_col, axis=1, inplace=True)
        df.dropna(how='all', axis=0, inplace=True)
        df.dropna(how='all', axis=1, inplace=True)
        return df, path

    def load_data(self, file_path=None, label_col=None, drop_col=None, force=False, mode='train', df=None):
        '''Load data through a file, drop the useless columns, save the feature and label.

        Parameters
        ----------
        file_path : str, default=None
            File path contains the data.

        label_col : str, default=None
            Column name of the label.

        drop_col : list, default=None
            The name of useless columns.

        force : bool, default=False
            Reload the data by force.

        mode : str, default='train'
            Mode can be 'train' or 'predict'.
            
        df : DataFrame, default=None
            If it is provided, the file_path will be ignored.
            Otherwise, the data will be loaded through file_path.
        '''
        # prepare the data
        if df is None:
            df, file_path = self._file_preprocess(file_path, drop_col)

        # set the training data
        if mode == 'train':
            self.load_training_data(file_path, label_col, drop_col, df)
        # set the predict data
        elif mode == 'predict':
            self.load_predict_data(file_path, label_col, drop_col, df)
        else:
            raise Exception('Key error for the mode:%s.' % mode)

        # clear the record of feature expansion
        if force:
            self.feature_expand = []

    def load_training_data(self, file_path=None, label_col=None, drop_col=None, df=None):
        '''Load the training data.
        
        Parameters
        ----------
        Almost the same as self.load_data().
        '''
        # load file
        if df is None:
            df, file_path = self._file_preprocess(file_path)
        else:
            _df = df.copy(deep=True)

        # set the predict data info
        self.file_path = file_path
        self.label_col = label_col

        # set the label, feature and columns
        self.df = df
        self.df_y = _df.pop(label_col)
        self.df_x = _df.values
        self.df_x_columns = list(df.columns)
        self._train_data_exist = True
        print('Number of data set: %s' % (_df.shape[0]))
        print('Number of features: %s' % (_df.shape[1]))

    def load_predict_data(self, file_path=None, label_col=None, drop_col=None, df=None):
        '''Load the data used to predict.
        
        Parameters
        ----------
        Almost the same as self.load_data()
        '''
        # load file
        if df is None:
            df, file_path = self._file_preprocess(file_path)

        # set the training data info
        self.predict_file_path = file_path
        self.predict_label_col = label_col

        # set the label, feature and columns
        self.predict_df = df.copy(deep=True)
        self.predict_df_y = df.pop(label_col)
        self.predict_df_x = df.values
        self.predict_df_x_columns = list(df.columns)
        self._predict_data_exist = True
        print('Number of predicted data set: %s' % (df.shape[0]))
        print('Number of predicted features: %s' % (df.shape[1]))

    def feature_by_polynomial(self, degree=2, mode='train', **kwargs):
        '''Feature engineer: expand the features by PolynomicalFeatures.
        
        Parameters
        ----------
        degree : int, default=False
            The degree of polynomial features. Only degree = 2 is supported.
            
        mode : str, default='train'
            Mode can be 'train' or others.
            
        kwargs : 
            Other parameters that will be passed into the PolynomialFeatures funtion.
        '''
        if degree == 2:
            self.poly = PolynomialFeatures(degree=degree, include_bias=False, **kwargs)
            self.df_x = self.poly.fit_transform(self.df_x)
            # get the names of features after PolynomialFeatures
            columns = list(self.df_x_columns)
            df_x_columns = columns.copy()
            n = len(columns)
            for i in range(0, n):
                for j in range(i, n):
                    df_x_columns.append('{}*{}'.format(columns[i], columns[j]))
            if not (len(df_x_columns) == len(self.poly.get_feature_names())):
                raise Exception('PolynomialFeature transform error')
            self.df_x_columns = df_x_columns
        print('Number of existing features %s' % (self.df_x.shape[1]))

        # track and log the feature expansion procedure
        if mode == 'train':
            self.feature_expand.append(('feature_by_polynomial', self.poly))
            kwargs.update({'degree': degree, 'include_bias': False})
            self.feature_expand_params['feature_by_polynomial'] = kwargs

    def feature_by_variance(self, var_thresh=0, mode='train'):
        '''Feature engineer: delete features with near-zero variance.
        
        Parameters
        ----------
        var_thresh : float, default=0
            Features with a low variance will be deleted. 
            
        mode : str, default='train'
            Mode can be 'train' or others.
        '''
        selector = VarianceThreshold(var_thresh)
        try:
            var_selector_train = selector.fit_transform(self.df_x)
        except ValueError:
            print('No feature with near-zero variance.')
            self.var_selector_mask = None
        else:
            self.var_selector_mask = selector.get_support()
            self.df_x = var_selector_train
            self.df_x_columns = [i[0] for i in zip(self.df_x_columns, selector.get_support()) if i[1]]
        print('Number of existing features %s' % (self.df_x.shape[1]))

        # track and log the feature expansion procedure
        if mode == 'train':
            self.feature_expand.append(('feature_by_variance', self.var_selector_mask))
            self.feature_expand_params['feature_by_variance'] = {'var_thresh': var_thresh}

    def feature_by_correlation(self, high_corr=0, labeltype='regression', mode='train'):
        '''Feature engineer: delete features that have a high correlation with others.
        
        Parameters
        ----------
        high_corr : float, default=False
            Features with a high correlation will be deleted. 
        
        labeltype : str, default='regression'
            'regression' or 'classification'
            If regression assigned, correlation will be caled as a metric to determine which feature will be retained.
            
        mode : str, default='train'
            Mode can be 'train' or others.
        '''
        # calculate the correlation coefficients
        np_corr = np.corrcoef(self.df_x, rowvar=False)
        np_corr_triu = np.abs(np.triu(np_corr, 1))
        np_corr_bool = np_corr_triu > high_corr
        df_corr_bool = pd.DataFrame(np_corr_bool, columns=self.df_x_columns, index=self.df_x_columns)
        df_data = pd.DataFrame(self.df_x, columns=self.df_x_columns)
        mask = np_corr_bool.any(axis=0)
        # the feature columns with corrcoef > threshold
        high_feat = [i[0] for i in zip(self.df_x_columns, mask) if i[1]]
        drop_set = set()
        for col in high_feat:
            # collect the feature pairs
            col_relates = df_corr_bool.index[df_corr_bool[col] == True]
            cal_cols = list(col_relates)
            cal_cols.append(col)
            
            # determine the features that will be deleted
            if labeltype == 'regression':
                # calculate the corrcoef of the feature and the label
                drop_dict = {}
                drop_dict = {cal_col: df_data[cal_col].corr(pd.Series(self.df_y)) for cal_col in cal_cols}
                # select the lower corrcoef and delete
                drop_set |= set([i[0] for i in sorted(drop_dict.items(), key=lambda d: (d[1], d[0]))[:-1]])
            elif labeltype == 'classification':
                drop_set |= set(sorted(cal_cols, reverse=False)[:-1])
        support = [False if (i in drop_set) else True for i in self.df_x_columns]
        self.high_corr_mask = support
        self.df_x = self.df_x[:, support]
        self.df_x_columns = [i[0] for i in zip(self.df_x_columns, support) if i[1]]
        print('Number of existing features %s' % (self.df_x.shape[1]))

        # track and log the feature expansion procedure
        if mode == 'train':
            self.feature_expand.append(('feature_by_correlation', self.high_corr_mask))
            self.feature_expand_params['feature_by_correlation'] = {'high_corr': high_corr}

    def feature_by_k_best(self, k_best, score_func, mode='train'):
        '''Feature engineer: select the k best features.
        
        Parameters
        ----------
        k_best : int
            Select the k best features.  
            
        score_func : callable
            The critieria for select features.
            eg. chi2, f_classif, f_regression
        
        mode : str, default='train'
            Mode can be 'train' or others.
        '''
        selector = SelectKBest(score_func, k=k_best)
        self.df_x = selector.fit_transform(self.df_x, self.df_y)
        self.k_best_mask = selector.get_support()
        self.df_x_columns = [i[0] for i in zip(self.df_x_columns, self.k_best_mask) if i[1]]
        print('Number of existing features %s' % (self.df_x.shape[1]))

        # track and log the feature expansion procedure
        if mode == 'train':
            self.feature_expand.append(('feature_by_k_best', self.k_best_mask))
            self.feature_expand_params['feature_by_k_best'] = {'k_best': k_best, 'score_func': score_func}
            
    def feature_by_percentile(self, percentile, score_func, mode='train'):
        '''Feature engineer: select features based on a percentile of the highest scores.
        
        Parameters
        ----------
        percentile : int
            The percentage of features to keep, default=10.  
            
        score_func : callable
            The critieria for select features.
            eg. chi2, f_classif, f_regression
        
        mode : str, default='train'
            Mode can be 'train' or others.
        '''
        selector = SelectPercentile(score_func, percentile=percentile)
        self.df_x = selector.fit_transform(self.df_x, self.df_y)
        self.percentile_mask = selector.get_support()
        self.df_x_columns = [i[0] for i in zip(self.df_x_columns, self.percentile_mask) if i[1]]
        print('Number of existing features %s' % (self.df_x.shape[1]))

        # track and log the feature expansion procedure
        if mode == 'train':
            self.feature_expand.append(('feature_by_percentile', self.percentile_mask))
            self.feature_expand_params['feature_by_percentile'] = {'percentile': percentile, 'score_func': score_func}

    def feature_by_model(self, estimator, threshold, mode='train', **kwargs):
        '''Feature engineer: select features from models.
        
        Parameters
        ----------
        estimator : callable
            Model used to select features which has "feature_importances_" or "coef_" attribute.
            eg. LinearRegression
            
        threshold : str
            The threshold value to use for feature selection.
            e.g. "1.25*mean"
        
        mode : str, default='train'
            Mode can be 'train' or others.
            
        kwargs : 
            Other parameters that will be passed into the SelectFromModel funtion.
        '''
        selector = SelectFromModel(estimator=estimator, threshold=threshold, **kwargs)
        self.df_x = selector.fit_transform(self.df_x, self.df_y)
        self.model_mask = selector.get_support()
        self.df_x_columns = [i[0] for i in zip(self.df_x_columns, self.model_mask) if i[1]]
        print('Number of existing features %s' % (self.df_x.shape[1]))

        # track and log the feature expansion procedure
        if mode == 'train':
            self.feature_expand.append(('feature_by_model', self.model_mask))
            kwargs.update({'estimator': estimator, 'threshold': threshold})
            self.feature_expand_params['feature_by_model'] = kwargs

    def feature_by_RFECV(self, estimator, step=1, min_features=1, mode='train', **kwargs):
        '''Feature engineer: Select features by boruta.
        
        Parameters
        ----------
        estimator : callable estimator
            An estimator with "fit" method and provide importance either through
            a "coef_" attribute or "feature_importances_" attribute.

        step : int or float, default=1
            The number of features to remove at each iteration.
            eg. 5
            The percentage of features to remove at each iteration.
            eg. 0.5

        min_features : int, default=1
            The minimum number of features to be selected.
        
        mode : str, default='train'
            Mode can be 'train' or others.
            
        kwargs : 
            Other parameters that will be passed into the RFECV funtion.
        '''
        selector = RFECV(estimator, step=step, min_features_to_select=min_features)
        self.df_x = selector.fit_transform(self.df_x, self.df_y)
        self.rfe_mask = selector.get_support()
        self.df_x_columns = [i[0] for i in zip(self.df_x_columns, self.rfe_mask) if i[1]]
        print('Number of existing features %s' % (self.df_x.shape[1]))

        # track and log the feature expansion procedure
        if mode == 'train':
            self.feature_expand.append(('feature_by_RFECV', self.rfe_mask))
            kwargs.update({'estimator': estimator, 'step': step, 'min_features': min_features})
            self.feature_expand_params['feature_by_RFECV'] = kwargs

    def feature_by_boruta(self, task, perc=100, ml_params={}, mode='train'):
        '''Feature engineer: Select features by using recursive feature elimination.
        
        Parameters
        ----------
        task : 'classification'(default) or 'regression'
            The parameter of boruta, to specify a supervised learning estimator.
        
        mode : str, default='train'
            Mode can be 'train' or others.
        '''
        if task == 'classification':
            self.task = 'classification'
            rf = RandomForestClassifier(n_jobs=self.n_jobs, **ml_params)
        if task == 'regression':
            self.task = 'regression'
            rf = RandomForestRegressor(n_jobs=self.n_jobs, **ml_params)
        selector = BorutaPy(rf, n_estimators='auto', perc=perc, verbose=0, random_state=0)
        selector.fit(self.df_x, self.df_y)
        self.boruta_mask = selector.support_
        self.df_x = self.df_x[:, selector.support_]
        self.df_x_columns = [i[0] for i in zip(self.df_x_columns, selector.support_) if i[1]]
        print('Number of existing features %s' % (self.df_x.shape[1]))

        # track and log the feature expansion procedure
        if mode == 'train':
            self.feature_expand.append(('feature_by_boruta', self.boruta_mask))
            self.feature_expand_params['feature_by_boruta'] = {'task': task}

    def feature_standardize(self, scale):
        ''' Feature standardize and feature scaling.

        scale : {'MinMaxScaler', 'StandardScaler', 'Normalizer'}, default=None
            Specifies the scale method to be used in the feature scaling.
        '''
        self.scaler = None
        self.scaler, self.df_x = standardize(scale, self.df_x)
        self.feature_expand.append(('feature_standardize', self.scaler))
        self.feature_expand_params['feature_standardize'] = {'scaler': self.scaler}

    def data_sampling(self, stratify: ['stratify', 'random'] = 'stratify', test_size=0.2, rand=1):
        '''Split the dataset into training set and test set.
        If this function is executed, the data will be split into training and test set.
        Otherwise, the whole dataset will be used to training the model.
        
        Parameters
        ----------
        stratify : str, default='stratify'
            Stratify can be 'stratify' or 'random'. 'Stratify' means data is splited in a stratified fashion,
            'random' means the data is splited randomly.
        
        test_size : float, default=0.2
            The proportion of the test set.
            
        rand : int, default=1
            Random state to split the data.
        '''
        # stratified sampling
        self.stratify = stratify
        if stratify == 'stratify':
            self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.df_x, self.df_y,
                                                                                    test_size=test_size,
                                                                                    stratify=self.df_y,
                                                                                    random_state=rand)
        elif stratify == 'random':
            self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.df_x, self.df_y,
                                                                                    test_size=test_size, shuffle=True,
                                                                                    random_state=rand)
        else:
            raise Exception('stratify key error')

        self._split = True
        print('number of training set: %s' % (self.x_train.shape[0]))
        print('number of test set: %s' % (self.x_test.shape[0]))

    def engineer_for_prediction(self):
        '''Feature engineer for the predicted data.'''
        for func in self.feature_expand:
            item = func[0]
            mask = func[1]
            if item == 'feature_by_polynomial':
                # PolynomialFeatures
                self.predict_df_x = mask.transform(self.predict_df_x)

            elif item == 'feature_by_variance':
                # near-zero variance
                if mask is not None:
                    self.predict_df_x = self.predict_df_x[:, mask]

            elif item == 'feature_by_correlation':
                # delete features that have a high correlation with others
                if mask is not None:
                    self.predict_df_x = self.predict_df_x[:, mask]

            elif item == 'feature_by_k_best':
                # select the k best features
                if mask is not None:
                    self.predict_df_x = self.predict_df_x[:, mask]

            elif item == 'feature_by_model':
                # select the features from model
                if mask is not None:
                    self.predict_df_x = self.predict_df_x[:, mask]

            elif item == 'feature_by_RFECV':
                # select the features by recursive feature elimination
                if mask is not None:
                    self.predict_df_x = self.predict_df_x[:, mask]

            elif item == 'feature_by_boruta':
                # select features using boruta
                if mask is not None:
                    self.predict_df_x = self.predict_df_x[:, mask]

            elif item == 'feature_by_percentile':
                # select features by percentile
                if mask is not None:
                    self.predict_df_x = self.predict_df_x[:, mask]
                    
            elif item == 'feature_standardize':
                # standardize
                if mask is not None:
                    self.predict_df_x = mask.transform(self.predict_df_x)
                    
            print('Number of final features of predicted data %s' % (self.predict_df_x.shape[1]))

    def save_data(self, fname=None, ftime=True):
        '''Save the model object and results.

        Parameters
        ----------
        fname : str, default=None
            The file used for binary writing.

        time : bool, default=True
            Add the current time to the file name.
        '''
        if fname:
            fname = str(fname)
        if ftime:
            time_now = time.strftime("%Y_%m_%d_%H_%M", time.localtime())
        else:
            time_now = ''

        name_comp = [i for i in [fname, time_now] if i]
        self.save_file = self.data_path / '{}.pkl'.format('_'.join(name_comp))

        output = open(self.save_file, 'wb')
        pickle.dump(self, output)
        output.close()


class MolecularData(Data):
    def _fp_generator(self, x):
        '''Fingerprints generator
        
        Parameters
        ----------
        x : rdmol object
            Mols from the rdkit
            
        Returns
        -------
        fp : fingerprints
            The calculated fingerprints.
        '''
        if self.fp_type == 'morgan':
            fp = GetMorganFingerprintAsBitVect(x, **self.fp_params)

        elif self.fp_type == 'RDK':
            fp = RDKFingerprint(x, **self.fp_params)

        elif self.fp_type == 'pairs':
            fp = Pairs.GetHashedAtomPairFingerprint(x, **self.fp_params)

        elif self.fp_type == 'torsion':
            fp = Torsions.GetHashedTopologicalTorsionFingerprint(x, **self.fp_params)

        elif self.fp_type == 'avalon':
            fp = GetAvalonFP(x, **self.fp_params)

        elif self.fp_type == 'EState':
            fp = FingerprintMol(x, **self.fp_params)[0]

        elif self.fp_type == 'MACCS':
            fp = MACCSkeys.GenMACCSKeys(x, **self.fp_params)

        return fp

    def _file_preprocess(self, file_path, drop_col=None, smiles_col=None, mw_min=0, mw_max=2000):
        '''Read the file, convert the smiles, return the data and the column name of rdmol.
        
        Parameters
        ----------
        file_path : str
            file path contains the data.

        drop_col : list, default=None
            The name of useless columns.

        smiles_col : bool, default=None
            The name of the smiles column.

        mw_min : int, default=0
            MW of the mols lower than than this threshold will be delete.
            
        mw_max : int, default=0
            MW of the mols larger than than this threshold will be delete.
        '''
        # load the file
        print('file_pre:', file_path)
        file_path = super()._valid_file(file_path)
        if file_path.suffix == '.xlsx':
            df = pd.read_excel(file_path)
        elif file_path.suffix == '.tsv':
            df = pd.read_csv(file_path, delimiter='\t')
        elif file_path.suffix == '.csv':
            df = pd.read_csv(file_path, delimiter=',')

        # delete the usless columns
        if drop_col:
            df.drop(drop_col, axis=1, inplace=True)
        df.dropna(how='all', axis=0, inplace=True)
        df.dropna(how='all', axis=1, inplace=True)
        df.dropna(subset=[smiles_col], inplace=True)

        # convert the smiles to rdmol
        rdmol_col = smiles2rdmol(df, smiles_col)

        # delete None
        df_error = df[df[rdmol_col] != df[rdmol_col]].copy()
        df_error['reason'] = 'unrecognize'
        df_error_index = df_error.index.tolist()
        df.drop(index=df_error_index, inplace=True)

        # delete mols out of range
        df['mw'] = df[rdmol_col].apply(Descriptors.MolWt)
        df_large = df[(df['mw'] > mw_max) & (df['mw'] < mw_min)].copy()
        df_large['reason'] = 'out of range'
        self.df_error = pd.concat([df_error, df_large], axis=0)
        df.drop(index=df_large.index.tolist(), inplace=True)
        return df, rdmol_col

    def load_data(self, rdmol_col=None, file_path=None, drop_col=None, smiles_col=None, mw_min=0, mw_max=2000, label_col=None,
                  id_col=None, force=False, mode='train', df=None):
        '''Load molecular data through a file, clean and convert the smiles to rdmol.
        
        Parameters
        ----------
        file_path : str, default=None
            Excel path contains the data.
            
        drop_col : list, default=None
            The name of useless columns.
            
        smiles_col : str, default=None
            Column name of the SMILES.
            
        mw_min : int, default=0
            The molecules with MW smaller than mw_min will be delete.
            
        mw_max : int, default=2000
            The molecules with MW larger than mw_max will be delete.
            
        label_col : string, default=None
            The name of the label columns.
            
        id_col : str, default=None
            Column name of the id.
            
        force : bool, deafault=False
            Clear the record of feature engineering process.

        mode : str, default='train'
            mode can be 'train' or 'predict'.
            
        df : DataFrame
            If file_path not provided, df will be used as training data.
        '''
        # prepare the data
        if df is None:
            df, rdmol_col = self._file_preprocess(file_path, drop_col, smiles_col, mw_min, mw_max)
            
        # prepare the smiles
        if rdmol_col is None:
            rdmol_col = smiles2rdmol(df, smiles_col)
            
        # set the training data
        if mode == 'train':
            self.load_training_data(rdmol_col, file_path, drop_col, smiles_col, mw_min, mw_max, label_col, id_col, df)
        # set the predicting data
        elif mode == 'predict':
            self.load_predict_data(rdmol_col, file_path, drop_col, smiles_col, mw_min, mw_max, label_col, id_col, df)

        # clear the record of feature expansion
        if force:
            self.feature_expand = []

    def load_training_data(self, rdmol_col=None, file_path=None, drop_col=None, smiles_col=None, mw_min=0, mw_max=2000,
                           label_col=None, id_col=None, df=None):
        '''Load the training data.
        
        Parameters
        ----------
        Almost the same as self.load_data()
        '''
        # load file
        if df is None:
            df, rdmol_col = self._file_preprocess(file_path, drop_col, smiles_col, mw_min, mw_max)
        else:
            _df = df.copy(deep=True)

        # save the info of training data
        self.rdmol_col = rdmol_col
        self.file_path = file_path
        self.smiles_col = smiles_col
        self.label_col = label_col
        self.id_col = id_col
        self.df = df
        self.df_smiles = df[[smiles_col]]
        if label_col:
            self.df_y = _df.pop(label_col)
        else:
            self.df_y = None
        self._train_data_exist = True
        print('Number of data set: %s' % (df.shape[0]))

    def load_predict_data(self, rdmol_col=None, file_path=None, drop_col=None, smiles_col=None, mw_min=0, mw_max=2000,
                          label_col=None, id_col=None, df=None):
        '''Load the data used to predict.
        
        Parameters
        ----------
        Almost the same as self.load_data()
        '''
        # load file
        if df is None:
            df, rdmol_col = self._file_preprocess(file_path, drop_col, smiles_col, mw_min, mw_max)
        else:
            _df = df.copy(deep=True)

        # todo: need change
        # save the info of training data
        self.rdmol_col = rdmol_col
        self.predict_file_path = file_path
        self.predict_smiles_col = smiles_col
        self.predict_label_col = label_col
        self.predict_id_col = id_col
        self.predict_df = df
        self.predict_df_smiles = df[[smiles_col]]
        if label_col:
            self.predict_df_y = _df.pop(label_col)
        else:
            self.predict_df_y = None
        self._predict_data_exist = True
        print('Number of predicted data set: %s' % (df.shape[0]))

    def feature_from_smiles(self, rdk_fp={}, rdk_descriptor=True, mode='train'):
        '''Three parts of features can be extracted: fps in rdkit, descriptors in rdkit, and descriptors in padel.
        
        Parameters
        ----------
        rdk_fp : dict
            The method and parameters for the feature extraction.
            eg. {'morgan':{'radius': 2}}
            
        rdk_descriptor : bool
            Calculate the rdkit descriptors or not.

        mode : str, default='train'
            Mode can be 'train' or 'predict'.
        '''
        # set the data and params
        if mode == 'train':
            self.rdk_fp = rdk_fp
            self.rdk_descriptor = rdk_descriptor
            df_temp = deepcopy(self.df)
            rdmol_col = self.rdmol_col
        elif mode == 'predict':
            rdk_fp = self.rdk_fp
            rdk_descriptor = self.rdk_descriptor
            df_temp = deepcopy(self.predict_df)
            rdmol_col = self.rdmol_col

        # extract features for the data
        time1 = time.time()
        temp_list = []
        if isinstance(rdk_fp, dict):
            if not rdk_fp:
                rdk_fp = {
                    'morgan':
                        {
                            'radius': 2,
                            'nBits': 1024,
                            'useFeatures': True
                        },
                    'RDK':
                        {
                            'minPath': 1,
                            'maxPath': 7,
                            'fpSize': 1024,
                            'useHs': True,
                            'tgtDensity': 0,
                            'minSize': 128
                        },
                    'pairs':
                        {
                            'nBits': 1024,
                            'minLength': 1,
                            'maxLength': 30,
                            'includeChirality': False,
                            'use2D': True
                        },
                    'torsion':
                        {
                            'nBits': 1024,
                            'targetSize': 4,
                            'includeChirality': False
                        },
                    'avalon':
                        {
                            'nBits': 1024
                        },
                    'EState': {},
                    'MACCS': {}
                }

            # calc the fps
            fp_available = ['morgan', 'RDK', 'pairs', 'torsion', 'avalon', 'EState', 'MACCS']
            self.rd_fp_list = []
            for fp in rdk_fp:
                if fp in fp_available:
                    print('calc %s...' % fp)
                    self.fp_type = fp
                    self.fp_params = rdk_fp[fp]
                    df_temp[fp] = df_temp[rdmol_col].map(self._fp_generator)
                    self.rd_fp_list.append(fp)

            # combine the fps
            for fp in self.rd_fp_list:
                df_temp[fp] = df_temp[fp].map(list)
                df_temp[fp] = df_temp[fp].apply(lambda x: str(x)[1:-1])
                feature_temp = df_temp[fp].str.split(',', expand=True)
                feature_temp.columns = ['%s_%s' % (fp, x) for x in range(feature_temp.shape[1])]
                temp_list.append(feature_temp)

        # calculate the descriptors in rdk
        if rdk_descriptor:
            if isinstance(rdk_descriptor, list):
                des_columns = rdk_descriptor
            else:
                des_columns = [desc_name[0] for desc_name in Descriptors._descList]
            print('calc descriptors...')
            des_calc = MoleculeDescriptors.MolecularDescriptorCalculator(des_columns)
            self.des_columns = des_columns
            self.des_calctor = des_calc
            df_temp['rdk_descriptor'] = df_temp[rdmol_col].map(des_calc.CalcDescriptors)
            df_temp['rdk_descriptor'] = df_temp['rdk_descriptor'].apply(lambda x: str(x)[1:-1])
            feature_temp = df_temp['rdk_descriptor'].str.split(',', expand=True)
            feature_temp.columns = des_columns
            temp_list.append(feature_temp)

        # combine the features and fps from rdk and padel
        self.temp_list = temp_list
        if len(temp_list) > 1:
            feature_temp = pd.concat(temp_list, axis=1).astype(np.float)
        else:
            feature_temp = temp_list[0].astype(np.float)

        # save the features (and the columns)
        if mode == 'train':
            self.df_x = feature_temp.values
            self.df_x_columns = feature_temp.columns
            print('Number of features for the training data: %s' % feature_temp.shape[1])
        elif mode == 'predict':
            self.predict_df_x = feature_temp.values
            print('Number of features for the predicted data: %s' % feature_temp.shape[1])

        time2 = time.time()
        print('Using time: %.2f\n' % (time2 - time1))
        del feature_temp, df_temp, temp_list
        gc.collect()

    def features_for_prediction(self):
        '''Generate the features for the predicted data'''
        self.feature_from_smiles(self, mode='predict')
